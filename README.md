# LLM 파인튜닝 및 RAG 기반 불공정 약관 탐지 시스템

## 프로젝트 개요
**Llama-3.1-8B** 모델을 기반으로 약관의 불공정성을 판별하고, **RAG** 기술을 적용하여 관련 법적 근거를 제시하는 AI기반 불공정 약관 탐지 시스템입니다.

## 방법론
1. **Model Fine-tuning:**
   - Base model: Llama-3.1-8B-Instruct 
   - QLoRA 방법론 적용
   - 불공정 여부 판별 및 43개 클래스의 계약 분야 분류 태스크 수행
2. **RAG:**
   - 불공정 약관에 대한 법령 검색 및 제시
   - 임베딩 모델 파인튜닝하여 성능 향상
   - SFT 모델이 생성한 '판단 근거'를 쿼리에 추가하여 검색 성능 고도화

---

## 평가 결과

### 1. AI-Hub 데이터 기반 10-Fold 교차 검증
전체 데이터셋을 10분할하여 학습 및 평가를 반복, 객관적인 일반화 성능을 검증하였습니다.
- **불공정 탐지:** SFT 모델은 Recall 0.95 수준을 기록하여 불공정 조항을 거의 완벽하게 탐지함.
- **분야 분류:** 43개 클래스의 분야 분류 작업 Macro-F1 score 향상 확인.

### 2. 한국공정거래조정원 분쟁사례 기반 외부 데이터 검증
학습에 참여하지 않은 실제 분쟁 사례를 통해 실용성을 최종 검증하였습니다.

불공정 약관 탐지 태스크 성능 검증 결과는 다음과 같습니다.
| 평가지표 (Metric) | Base Model | SFT Model | 증감 (Improvement) |
| :--- | :---: | :---: | :---: |
| **Accuracy** | 0.5417 | **0.8958** | ▲ 0.3541 |
| **Recall** | 0.5417 | **0.8958** | ▲ 0.3541 |
| **F1 Score** | 0.7027 | **0.9451** | ▲ 0.2424 |

SFT 모델은 실제 현실 데이터에서도 Recall 성능 향상을 유지하며 일반화 성능을 입증했습니다.

### 3. RAG 검색 성능 평가
**1) Retriever 파인튜닝 효과**
한국어 법률 도메인 데이터로 임베딩 모델(`nlpai-lab/KURE-v1`)을 파인튜닝하여 검색 정확도를 비약적으로 개선했습니다.
- **Recall@1:** 0.0889 (Base) → **0.3778 (Tuned)**

**2) 쿼리 확장(Query Expansion) 효과**
파인튜닝된 모델에 SFT 모델이 추론한 '판단 근거'를 쿼리에 추가하여 성능을 고도화했습니다.
- **Recall@1:** 0.3778 → **0.3926** 
- **MRR:** 0.5235 → **0.5332**